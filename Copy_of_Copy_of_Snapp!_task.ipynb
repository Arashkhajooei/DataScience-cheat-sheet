{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOPKdSaArZo0o/GzHWYsm9b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arashkhgit/DataScience-cheat-sheet/blob/main/Copy_of_Copy_of_Snapp!_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OU6Nj9sZUC5j"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "chunk_size = 10000\n",
        "data_chunks = pd.read_csv('/content/dataset_orders.csv', chunksize=chunk_size)\n",
        "data = pd.concat(data_chunks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning and Preprocessing:\n",
        "\n",
        "\n",
        "\n",
        "*   Handle missing data: Identify missing values and decide how to deal with them (e.g., impute, drop, or leave as-is).\n",
        "*   Remove duplicate records, if any, to avoid skewing analysis results.\n",
        "*   Address data inconsistencies, such as spelling errors, capitalization, or data entry mistakes.\n",
        "*   Handle outliers if they exist and could influence the analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QAwG8kewcznm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Data Examination"
      ],
      "metadata": {
        "id": "CTbycLgHd4tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the basic information of the data\n",
        "print(\"Step 1: Data Information\")\n",
        "print(data.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMaFwLkjawkS",
        "outputId": "07763a1c-7e58-4363-9277-0214ea53273f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Data Information\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 81700 entries, 0 to 81699\n",
            "Data columns (total 11 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   order_id      81700 non-null  int64  \n",
            " 1   create_time   81699 non-null  object \n",
            " 2   category      81699 non-null  object \n",
            " 3   city          81699 non-null  object \n",
            " 4   customer_id   81699 non-null  float64\n",
            " 5   price         81699 non-null  float64\n",
            " 6   distance      81699 non-null  float64\n",
            " 7   status        81699 non-null  object \n",
            " 8   cancelled_by  35760 non-null  object \n",
            " 9   biker_id      55831 non-null  float64\n",
            " 10  accept_time   81699 non-null  object \n",
            "dtypes: float64(4), int64(1), object(6)\n",
            "memory usage: 6.9+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Missing Value Analysis"
      ],
      "metadata": {
        "id": "5_o97yFNeLvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in each column\n",
        "print(\"\\nStep 2: Missing Values\")\n",
        "missing_values = data.isnull().sum()\n",
        "print(missing_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0caK_0uHd2hp",
        "outputId": "545087be-41ea-462d-d503-a9f8e7126c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2: Missing Values\n",
            "order_id            0\n",
            "create_time         1\n",
            "category            1\n",
            "city                1\n",
            "customer_id         1\n",
            "price               1\n",
            "distance            1\n",
            "status              1\n",
            "cancelled_by    45940\n",
            "biker_id        25869\n",
            "accept_time         1\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.1: Check for data duplication"
      ],
      "metadata": {
        "id": "aB7-8mOnffcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for data duplication\n",
        "print(\"\\nStep 3: Data Duplication Check\")\n",
        "duplicated_rows = data.duplicated()\n",
        "duplicated_count = duplicated_rows.sum()\n",
        "print(\"Number of duplicated rows:\", duplicated_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVQmnCLrfb-j",
        "outputId": "dce3cb79-07fe-4760-87f7-3f8a558358c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3: Data Duplication Check\n",
            "Number of duplicated rows: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Data Cleaning"
      ],
      "metadata": {
        "id": "AvhVNc7ueP5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3.1: Convert date columns to proper datetime format\n",
        "data['create_time'] = pd.to_datetime(data['create_time'])\n",
        "data['accept_time'] = pd.to_datetime(data['accept_time'])\n",
        "\n",
        "# Step 3.2: Convert 'price' and 'Distance' columns to numeric (if they contain numeric values)\n",
        "data['price'] = pd.to_numeric(data['price'], errors='coerce')\n",
        "data['distance'] = pd.to_numeric(data['distance'], errors='coerce')\n",
        "\n",
        "# Step 3.3: Handle missing values in 'biker_id'\n",
        "# Since 'biker_id' is empty when the order is not accepted, let's fill it with a more appropriate value, such as 'NOT_ACCEPTED'\n",
        "data['biker_id'].fillna('NOT_ACCEPTED', inplace=True)\n",
        "\n",
        "# Step 3.4: Handle missing values in 'cancelled_by'\n",
        "# Fill missing values in 'cancelled_by' with \"UNKNOWN\"\n",
        "data['cancelled_by'].fillna(\"UNKNOWN\", inplace=True)"
      ],
      "metadata": {
        "id": "AJp8krlreS-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Data Preprocessing"
      ],
      "metadata": {
        "id": "fHiildNafXSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4.1: Convert text columns to lowercase\n",
        "data['category'] = data['category'].str.lower()\n",
        "data['city'] = data['city'].str.lower()\n",
        "data['status'] = data['status'].str.lower()\n",
        "data['cancelled_by'] = data['cancelled_by'].str.lower()\n",
        "\n",
        "# Step 4.2: Remove duplicate rows, if any\n",
        "data.drop_duplicates(inplace=True)\n",
        "\n",
        "# Step 4.3: Convert 'order_id', 'customer_id', and 'biker_id' to integers\n",
        "data['order_id'] = data['order_id'].astype(int)\n",
        "data['customer_id'] = data['customer_id'].astype(int)"
      ],
      "metadata": {
        "id": "7KhdI3Bsed0v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "5d36580a-780c-419a-b7f0-a9e1cdb85baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IntCastingNaNError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIntCastingNaNError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-238e69fa1520>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Step 4.3: Convert 'order_id', 'customer_id', and 'biker_id' to integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'order_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'order_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'customer_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'customer_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6239\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6240\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6241\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     def convert(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;31m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;31m# in pandas we don't store numpy str dtypes, so convert to object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_astype_float_to_int_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36m_astype_float_to_int_nansafe\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \"\"\"\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         raise IntCastingNaNError(\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0;34m\"Cannot convert non-finite values (NA or inf) to integer\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         )\n",
            "\u001b[0;31mIntCastingNaNError\u001b[0m: Cannot convert non-finite values (NA or inf) to integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = data.isnull().sum()\n",
        "print(missing_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaEgsozCeeC4",
        "outputId": "c1871ed9-2321-4171-d628-5e6db71825b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "order_id        0\n",
            "create_time     1\n",
            "category        1\n",
            "city            1\n",
            "customer_id     1\n",
            "price           1\n",
            "distance        1\n",
            "status          1\n",
            "cancelled_by    0\n",
            "biker_id        0\n",
            "accept_time     1\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "ONCPpEvJgHRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4.1: Summary Statistics\n",
        "print(\"\\nStep 4.1: Summary Statistics\")\n",
        "print(data.describe())\n",
        "\n",
        "# Step 4.2: Category-wise Analysis\n",
        "print(\"\\nStep 4.2: Category-wise Analysis\")\n",
        "category_counts = data['category'].value_counts()\n",
        "print(category_counts)\n",
        "\n",
        "# Step 4.3: City-wise Analysis\n",
        "print(\"\\nStep 4.3: City-wise Analysis\")\n",
        "city_counts = data['city'].value_counts()\n",
        "print(city_counts)\n",
        "\n",
        "# Step 4.4: Status-wise Analysis\n",
        "print(\"\\nStep 4.4: Status-wise Analysis\")\n",
        "status_counts = data['status'].value_counts()\n",
        "print(status_counts)\n",
        "\n",
        "# Step 4.5: Cancellation Reasons Analysis\n",
        "print(\"\\nStep 4.5: Cancellation Reasons Analysis\")\n",
        "cancelled_by_counts = data['cancelled_by'].value_counts()\n",
        "print(cancelled_by_counts)\n",
        "\n",
        "# Step 4.6: Price and Distance Analysis\n",
        "print(\"\\nStep 4.6: Price and Distance Analysis\")\n",
        "print(\"Average Price:\", data['price'].mean())\n",
        "print(\"Maximum Price:\", data['price'].max())\n",
        "print(\"Minimum Price:\", data['price'].min())\n",
        "print(\"Average Distance:\", data['distance'].mean())\n",
        "print(\"Maximum Distance:\", data['distance'].max())\n",
        "print(\"Minimum Distance:\", data['distance'].min())"
      ],
      "metadata": {
        "id": "rPbrdaL9gKgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Data Visualization"
      ],
      "metadata": {
        "id": "dJwFoFr3g2n9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with 3 subplots\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Category-wise Bar Plot\n",
        "axs[0].bar(category_counts.index, category_counts.values, color='skyblue')\n",
        "axs[0].set_xlabel('Category')\n",
        "axs[0].set_ylabel('Count')\n",
        "axs[0].set_title('Category-wise Distribution of Orders')\n",
        "axs[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# City-wise Bar Plot\n",
        "axs[1].bar(city_counts.index, city_counts.values, color='orange')\n",
        "axs[1].set_xlabel('City')\n",
        "axs[1].set_ylabel('Count')\n",
        "axs[1].set_title('City-wise Distribution of Orders')\n",
        "axs[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Status Pie Chart\n",
        "axs[2].pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])\n",
        "axs[2].set_title('Status-wise Distribution of Orders')\n",
        "\n",
        "# Adjust layout and display the combined chart\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F-kkJQx3g9ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Matrix and Heatmap\n",
        "correlation_matrix = data.corr()\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y2CkjXIoiSlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Data validation"
      ],
      "metadata": {
        "id": "-bGL_hrRkjtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data validation is a critical step to ensure that the data cleaning and preprocessing have been performed correctly and that the data meets quality standards. In this process, we will cross-check the preprocessed data against the original data or known data to verify its correctness. Let's perform data validation for your preprocessed data:"
      ],
      "metadata": {
        "id": "tOfPKrPwkixf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Load the original data into a DataFrame\n",
        "chunk_size = 10000\n",
        "data_chunks = pd.read_csv('/content/dataset_orders.csv', chunksize=chunk_size)\n",
        "original_data = pd.concat(data_chunks)\n",
        "\n",
        "# Save the preprocessed data to a CSV file\n",
        "data.to_csv('/content/preprocessed_data.csv', index=False)\n",
        "\n",
        "# Read the preprocessed data from the saved CSV file\n",
        "preprocessed_data = pd.read_csv('/content/preprocessed_data.csv')"
      ],
      "metadata": {
        "id": "Ire2tXnWlVkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6.1: Compare the number of rows in original and preprocessed data\n",
        "original_rows, original_cols = original_data.shape\n",
        "preprocessed_rows, preprocessed_cols = preprocessed_data.shape\n",
        "\n",
        "if original_rows == preprocessed_rows:\n",
        "    print(\"Step 6.1: Number of rows in original and preprocessed data match.\")\n",
        "else:\n",
        "    print(\"Step 6.1: Number of rows in original and preprocessed data do not match. Check for potential issues.\")\n",
        "\n",
        "# Step 6.2: Compare the columns in original and preprocessed data\n",
        "original_columns = set(original_data.columns)\n",
        "preprocessed_columns = set(preprocessed_data.columns)\n",
        "\n",
        "if original_columns == preprocessed_columns:\n",
        "    print(\"Step 6.2: Columns in original and preprocessed data match.\")\n",
        "else:\n",
        "    print(\"Step 6.2: Columns in original and preprocessed data do not match. Check for potential issues.\")\n",
        "\n",
        "# Step 6.3: Check for any missing values in preprocessed data\n",
        "missing_values_preprocessed = preprocessed_data.isnull().sum().sum()\n",
        "\n",
        "if missing_values_preprocessed == 0:\n",
        "    print(\"Step 6.3: No missing values in the preprocessed data.\")\n",
        "else:\n",
        "    print(f\"Step 6.3: Preprocessed data contains {missing_values_preprocessed} missing values.\")\n",
        "\n",
        "# Step 6.4: Verify data transformation and cleaning results for specific columns\n",
        "# (You can cross-check specific columns if needed)\n",
        "\n",
        "# Example: Verify the uniqueness of 'order_id' in the preprocessed data\n",
        "if preprocessed_data['order_id'].nunique() == preprocessed_rows:\n",
        "    print(\"Step 6.4: 'order_id' is unique in the preprocessed data.\")\n",
        "else:\n",
        "    print(\"Step 6.4: 'order_id' is not unique in the preprocessed data. Check for potential issues.\")\n"
      ],
      "metadata": {
        "id": "CJBxhdBmikq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis Plan\n",
        "*   Define our objectve and questions\n",
        "*   Plan the analysis process and the techniques\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9V2_lKKPl9-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our first objective is that find \"Average order Time for Each Category\"**  :"
      ],
      "metadata": {
        "id": "uRBkyOVXoeKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Load the preprocessed data into a DataFrame\n",
        "chunk_size = 10000\n",
        "data_chunks = pd.read_csv('/content/preprocessed_data.csv', chunksize=chunk_size)\n",
        "data = pd.concat(data_chunks)\n",
        "\n",
        "# Step 1: Data Cleaning (already performed in the previous steps)\n",
        "\n",
        "# Step 2: Calculate the average time for each category\n",
        "\n",
        "# Convert 'create_time' and 'accept_time' to datetime objects\n",
        "data['create_time'] = pd.to_datetime(data['create_time'])\n",
        "data['accept_time'] = pd.to_datetime(data['accept_time'])\n",
        "\n",
        "# Filter out rows where 'accept_time' is before 'create_time'\n",
        "valid_data = data[data['accept_time'] >= data['create_time']].copy()\n",
        "\n",
        "# Calculate the time difference between 'create_time' and 'accept_time' in minutes for valid data\n",
        "valid_data['time_difference'] = (valid_data['accept_time'] - valid_data['create_time']).dt.total_seconds() / 60\n",
        "\n",
        "# Group the valid data by 'category' and calculate the average time difference for each category\n",
        "average_time_per_category = valid_data.groupby('category')['time_difference'].mean()\n",
        "\n",
        "# Convert the average time in minutes to minutes and seconds format\n",
        "average_time_per_category = pd.to_datetime(average_time_per_category, unit='m').dt.strftime('%M:%S')\n",
        "\n",
        "print(\"Average order Time for Each Category (minutes:seconds):\")\n",
        "print(average_time_per_category)"
      ],
      "metadata": {
        "id": "D5Uqu0Pkmbkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Load the preprocessed data into a DataFrame\n",
        "chunk_size = 10000\n",
        "data_chunks = pd.read_csv('/content/preprocessed_data.csv', chunksize=chunk_size)\n",
        "data = pd.concat(data_chunks)\n",
        "\n",
        "# Step 1: Data Cleaning (already performed in the previous steps)\n",
        "\n",
        "# Step 2: Data Preparation for Hourly Trend Plot\n",
        "\n",
        "# Convert 'create_time' to datetime object\n",
        "data['create_time'] = pd.to_datetime(data['create_time'])\n",
        "\n",
        "# Extract hour from 'create_time' to create a new 'hour' column\n",
        "data['hour'] = data['create_time'].dt.hour\n",
        "\n",
        "# Step 3: Data Preparation for Daily Trend Plot\n",
        "\n",
        "# Extract date from 'create_time' to create a new 'date' column\n",
        "data['date'] = data['create_time'].dt.date\n",
        "\n",
        "# Step 4: Hourly Trend Plot\n",
        "\n",
        "# Group the data by 'hour' and 'category' to get the count of requests for each hour in each category\n",
        "hourly_count_per_category = data.groupby(['hour', 'category']).size().reset_index(name='count')\n",
        "\n",
        "# Plot the hourly trend for each category\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=hourly_count_per_category, x='hour', y='count', hue='category', marker='o', linewidth=2)\n",
        "plt.title('Hourly Trend of Requests by Category')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Number of Requests')\n",
        "plt.xticks(range(24))\n",
        "plt.grid(True)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Daily Trend Plot\n",
        "\n",
        "# Group the data by 'date' and 'category' to get the count of requests for each day in each category\n",
        "daily_count_per_category = data.groupby(['date', 'category']).size().reset_index(name='count')\n",
        "\n",
        "# Plot the daily trend for each category\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=daily_count_per_category, x='date', y='count', hue='category', marker='o', linewidth=2)\n",
        "plt.title('Daily Trend of Requests by Category')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Requests')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aWWY2vektaoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Load the preprocessed data into a DataFrame\n",
        "chunk_size = 10000\n",
        "data_chunks = pd.read_csv('/content/preprocessed_data.csv', chunksize=chunk_size)\n",
        "data = pd.concat(data_chunks)\n",
        "\n",
        "# Step 1: Data Cleaning (already performed in the previous steps)\n",
        "\n",
        "# Step 2: Data Preparation for Hourly Trend Plot\n",
        "\n",
        "# Convert 'create_time' to datetime object\n",
        "data['create_time'] = pd.to_datetime(data['create_time'])\n",
        "\n",
        "# Extract hour from 'create_time' to create a new 'hour' column\n",
        "data['hour'] = data['create_time'].dt.hour\n",
        "\n",
        "# Step 3: Data Preparation for Daily Trend Plot\n",
        "\n",
        "# Extract date from 'create_time' to create a new 'date' column\n",
        "data['date'] = data['create_time'].dt.date\n",
        "\n",
        "# Step 4: Hourly Trend Outlier Detection (Z-score method)\n",
        "\n",
        "hourly_count_per_category = data.groupby(['hour', 'category']).size().reset_index(name='count')\n",
        "\n",
        "# Calculate Z-score for 'count' column within each category\n",
        "hourly_count_per_category['z_score'] = (hourly_count_per_category.groupby('category')['count']\n",
        "                                       .transform(lambda x: (x - x.mean()) / x.std()))\n",
        "\n",
        "# Identify potential outliers with Z-score > 3 or Z-score < -3\n",
        "hourly_outliers_zscore = hourly_count_per_category[(hourly_count_per_category['z_score'] > 3) |\n",
        "                                                   (hourly_count_per_category['z_score'] < -3)]\n",
        "\n",
        "print(\"Hourly Trend Outliers (Z-score method):\")\n",
        "print(hourly_outliers_zscore)\n",
        "\n",
        "# Step 5: Daily Trend Outlier Detection (Z-score method)\n",
        "\n",
        "daily_count_per_category = data.groupby(['date', 'category']).size().reset_index(name='count')\n",
        "\n",
        "# Calculate Z-score for 'count' column within each category\n",
        "daily_count_per_category['z_score'] = (daily_count_per_category.groupby('category')['count']\n",
        "                                      .transform(lambda x: (x - x.mean()) / x.std()))\n",
        "\n",
        "# Identify potential outliers with Z-score > 3 or Z-score < -3\n",
        "daily_outliers_zscore = daily_count_per_category[(daily_count_per_category['z_score'] > 3) |\n",
        "                                                 (daily_count_per_category['z_score'] < -3)]\n",
        "\n",
        "print(\"Daily Trend Outliers (Z-score method):\")\n",
        "print(daily_outliers_zscore)\n",
        "\n",
        "# Step 6: Hourly Trend Outlier Detection (IQR method)\n",
        "\n",
        "# Calculate the first quartile (Q1), third quartile (Q3), and interquartile range (IQR) for 'count' within each category\n",
        "hourly_summary_iqr = (hourly_count_per_category.groupby('category')['count']\n",
        "                      .agg(['quantile', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)])\n",
        "                      .rename(columns={'<lambda_0>': 'Q1', '<lambda_1>': 'Q3', 'quantile': 'median'}))\n",
        "\n",
        "# Calculate the lower and upper bounds for potential outliers\n",
        "hourly_summary_iqr['lower_bound'] = hourly_summary_iqr['Q1'] - 1.5 * (hourly_summary_iqr['Q3'] - hourly_summary_iqr['Q1'])\n",
        "hourly_summary_iqr['upper_bound'] = hourly_summary_iqr['Q3'] + 1.5 * (hourly_summary_iqr['Q3'] - hourly_summary_iqr['Q1'])\n",
        "\n",
        "# Identify potential outliers based on the bounds\n",
        "hourly_outliers_iqr = pd.merge(hourly_count_per_category, hourly_summary_iqr[['lower_bound', 'upper_bound']],\n",
        "                              on='category', how='left')\n",
        "hourly_outliers_iqr = hourly_outliers_iqr[\n",
        "    (hourly_outliers_iqr['count'] < hourly_outliers_iqr['lower_bound']) |\n",
        "    (hourly_outliers_iqr['count'] > hourly_outliers_iqr['upper_bound'])\n",
        "]\n",
        "\n",
        "print(\"Hourly Trend Outliers (IQR method):\")\n",
        "print(hourly_outliers_iqr)\n",
        "\n",
        "# Step 7: Daily Trend Outlier Detection (IQR method)\n",
        "\n",
        "# Calculate the first quartile (Q1), third quartile (Q3), and interquartile range (IQR) for 'count' within each category\n",
        "daily_summary_iqr = (daily_count_per_category.groupby('category')['count']\n",
        "                     .agg(['quantile', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)])\n",
        "                     .rename(columns={'<lambda_0>': 'Q1', '<lambda_1>': 'Q3', 'quantile': 'median'}))\n",
        "\n",
        "# Calculate the lower and upper bounds for potential outliers\n",
        "daily_summary_iqr['lower_bound'] = daily_summary_iqr['Q1'] - 1.5 * (daily_summary_iqr['Q3'] - daily_summary_iqr['Q1'])\n",
        "daily_summary_iqr['upper_bound'] = daily_summary_iqr['Q3'] + 1.5 * (daily_summary_iqr['Q3'] - daily_summary_iqr['Q1'])\n",
        "\n",
        "# Identify potential outliers based on the bounds\n",
        "daily_outliers_iqr = pd.merge(daily_count_per_category, daily_summary_iqr[['lower_bound', 'upper_bound']],\n",
        "                             on='category', how='left')\n",
        "daily_outliers_iqr = daily_outliers_iqr[\n",
        "    (daily_outliers_iqr['count'] < daily_outliers_iqr['lower_bound']) |\n",
        "    (daily_outliers_iqr['count'] > daily_outliers_iqr['upper_bound'])\n",
        "]\n",
        "\n",
        "print(\"Daily Trend Outliers (IQR method):\")\n",
        "print(daily_outliers_iqr)\n"
      ],
      "metadata": {
        "id": "MRWtXRoFuhFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Objective :\n",
        "we want to predict the number of orders (DELIVERED) in the future.\n",
        "How many trips will we have on each day of the first week\n",
        "of May?"
      ],
      "metadata": {
        "id": "2fP1FR5rwSPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARIMA"
      ],
      "metadata": {
        "id": "MAXzu3i01tU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# Step 0: Load the preprocessed data into a DataFrame\n",
        "chunk_size = 10000\n",
        "data_chunks = pd.read_csv('/content/preprocessed_data.csv', chunksize=chunk_size)\n",
        "data = pd.concat(data_chunks)\n",
        "\n",
        "# Step 1: Data Preparation\n",
        "\n",
        "# Convert 'create_time' to datetime object\n",
        "data['create_time'] = pd.to_datetime(data['create_time'])\n",
        "\n",
        "# Filter only the delivered orders\n",
        "delivered_data = data[data['status'] == 'delivered']\n",
        "\n",
        "# Set 'create_time' as the index to convert the DataFrame into a time series\n",
        "delivered_data.set_index('create_time', inplace=True)\n",
        "\n",
        "# Step 2: Resample the Data on a Daily Basis\n",
        "\n",
        "# Resample the data on a daily basis and get the count of delivered orders for each day\n",
        "daily_orders = delivered_data.resample('D').size()\n",
        "\n",
        "# Step 3: Train the ARIMA Model for Historical Data\n",
        "\n",
        "# Select the historical data for April 2020\n",
        "historical_data = daily_orders['2020-04-01':'2020-04-30']\n",
        "\n",
        "# Check if the historical data is sufficient for model training\n",
        "if len(historical_data) < 2:\n",
        "    print(\"Insufficient data for model training. Please ensure data includes at least two days of historical data.\")\n",
        "else:\n",
        "    # Train the ARIMA model on the historical data\n",
        "    try:\n",
        "        arima_model = ARIMA(historical_data, order=(1, 0, 0))  # You can modify the order as needed\n",
        "        arima_model_fit = arima_model.fit()\n",
        "\n",
        "        # Step 4: Forecasting for the Month of May 2020\n",
        "\n",
        "        # Generate the dates for the month of May 2020\n",
        "        forecast_start_date = pd.to_datetime('2020-05-01')\n",
        "        forecast_end_date = pd.to_datetime('2020-05-7')\n",
        "        forecast_dates = pd.date_range(start=forecast_start_date, end=forecast_end_date, freq='D')\n",
        "\n",
        "        # Use the trained ARIMA model to forecast the number of orders for the month of May 2020\n",
        "        forecast_values = arima_model_fit.forecast(steps=len(forecast_dates))\n",
        "\n",
        "        # Plot the forecasted values\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        plt.plot(historical_data.index, historical_data.values, label='Historical Data')\n",
        "        plt.plot(forecast_dates, forecast_values, label='Forecasted Values', marker='o')\n",
        "        plt.title('Forecast of Delivered Orders for the Month of May 2020')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Number of Orders')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.xticks(rotation=45, ha='right')  # Rotate date labels for readability\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Forecasted Number of Orders for the Month of May 2020:\")\n",
        "        print(forecast_values)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred during ARIMA model training:\", str(e))\n",
        "\n",
        "# Additional comments:\n",
        "# - The code now uses \"delivered\" instead of \"DELIVERED\" for consistency.\n",
        "# - The code includes historical data for April 2020 and uses ARIMA to forecast the number of orders for May 2020.\n",
        "# - The forecasted values for the month of May 2020 are displayed and plotted in the chart.\n"
      ],
      "metadata": {
        "id": "agBccBvCvFu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seasonal Autoregressive Integrated Moving-Average (SARIMA)\n",
        "SARIMA is an extension of ARIMA that considers seasonal components in addition to autoregressive and moving-average components. It is useful for time series data with seasonality."
      ],
      "metadata": {
        "id": "xZVaEg8Y1vWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# Step 0: Load the preprocessed data into a DataFrame\n",
        "chunk_size = 10000\n",
        "data_chunks = pd.read_csv('/content/preprocessed_data.csv', chunksize=chunk_size)\n",
        "data = pd.concat(data_chunks)\n",
        "\n",
        "# Step 1: Data Preparation\n",
        "\n",
        "# Convert 'create_time' to datetime object\n",
        "data['create_time'] = pd.to_datetime(data['create_time'])\n",
        "\n",
        "# Filter only the delivered orders\n",
        "delivered_data = data[data['status'] == 'delivered']\n",
        "\n",
        "# Set 'create_time' as the index to convert the DataFrame into a time series\n",
        "delivered_data.set_index('create_time', inplace=True)\n",
        "\n",
        "# Step 2: Resample the Data on a Daily Basis\n",
        "\n",
        "# Resample the data on a daily basis and get the count of delivered orders for each day\n",
        "daily_orders = delivered_data.resample('D').size()\n",
        "\n",
        "# Step 3: Train the SARIMA Model for Historical Data\n",
        "\n",
        "# Select the historical data for April 2020\n",
        "historical_data = daily_orders['2020-04-01':'2020-04-30']\n",
        "\n",
        "# Check if the historical data is sufficient for model training\n",
        "if len(historical_data) < 2:\n",
        "    print(\"Insufficient data for model training. Please ensure data includes at least two days of historical data.\")\n",
        "else:\n",
        "    # Train the SARIMA model on the historical data\n",
        "    try:\n",
        "        sarima_model = SARIMAX(historical_data, order=(1, 0, 0), seasonal_order=(1, 0, 0, 7))  # You can modify the order as needed\n",
        "        sarima_model_fit = sarima_model.fit()\n",
        "\n",
        "        # Step 4: Forecasting for the Month of May 2020\n",
        "\n",
        "        # Generate the dates for the month of May 2020\n",
        "        forecast_start_date = pd.to_datetime('2020-05-01')\n",
        "        forecast_end_date = pd.to_datetime('2020-05-31')\n",
        "        forecast_dates = pd.date_range(start=forecast_start_date, end=forecast_end_date, freq='D')\n",
        "\n",
        "        # Use the trained SARIMA model to forecast the number of orders for the month of May 2020\n",
        "        forecast_values = sarima_model_fit.forecast(steps=len(forecast_dates))\n",
        "\n",
        "        # Plot the forecasted values\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        plt.plot(historical_data.index, historical_data.values, label='Historical Data')\n",
        "        plt.plot(forecast_dates, forecast_values, label='Forecasted Values', marker='o')\n",
        "        plt.title('SARIMA Forecast of Delivered Orders for the Month of May 2020')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Number of Orders')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.xticks(rotation=45, ha='right')  # Rotate date labels for readability\n",
        "        plt.show()\n",
        "\n",
        "        print(\"SARIMA Forecasted Number of Orders for the Month of May 2020:\")\n",
        "        print(forecast_values)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred during SARIMA model training:\", str(e))\n"
      ],
      "metadata": {
        "id": "DdAOXVce1vBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exponential Smoothing (ETS):\n",
        "ETS methods are based on weighted averages of past observations to make predictions. They can handle data with or without seasonality and trend."
      ],
      "metadata": {
        "id": "9ERAan9X2DZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "\n",
        "# Step 0: Load the preprocessed data into a DataFrame\n",
        "chunk_size = 10000\n",
        "data_chunks = pd.read_csv('/content/preprocessed_data.csv', chunksize=chunk_size)\n",
        "data = pd.concat(data_chunks)\n",
        "\n",
        "# Step 1: Data Preparation\n",
        "\n",
        "# Convert 'create_time' to datetime object\n",
        "data['create_time'] = pd.to_datetime(data['create_time'])\n",
        "\n",
        "# Filter only the delivered orders\n",
        "delivered_data = data[data['status'] == 'delivered']\n",
        "\n",
        "# Set 'create_time' as the index to convert the DataFrame into a time series\n",
        "delivered_data.set_index('create_time', inplace=True)\n",
        "\n",
        "# Step 2: Resample the Data on a Daily Basis\n",
        "\n",
        "# Resample the data on a daily basis and get the count of delivered orders for each day\n",
        "daily_orders = delivered_data.resample('D').size()\n",
        "\n",
        "# Step 3: Train the ETS Model for Historical Data\n",
        "\n",
        "# Select the historical data for April 2020\n",
        "historical_data = daily_orders['2020-04-01':'2020-04-30']\n",
        "\n",
        "# Check if the historical data is sufficient for model training\n",
        "if len(historical_data) < 2:\n",
        "    print(\"Insufficient data for model training. Please ensure data includes at least two days of historical data.\")\n",
        "else:\n",
        "    # Train the ETS model on the historical data\n",
        "    try:\n",
        "        ets_model = ExponentialSmoothing(historical_data, seasonal='add', seasonal_periods=7)\n",
        "        ets_model_fit = ets_model.fit()\n",
        "\n",
        "        # Step 4: Forecasting for the Month of May 2020\n",
        "\n",
        "        # Generate the dates for the month of May 2020\n",
        "        forecast_start_date = pd.to_datetime('2020-05-01')\n",
        "        forecast_end_date = pd.to_datetime('2020-05-31')\n",
        "        forecast_dates = pd.date_range(start=forecast_start_date, end=forecast_end_date, freq='D')\n",
        "\n",
        "        # Use the trained ETS model to forecast the number of orders for the month of May 2020\n",
        "        forecast_values = ets_model_fit.forecast(len(forecast_dates))\n",
        "\n",
        "        # Plot the forecasted values\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        plt.plot(historical_data.index, historical_data.values, label='Historical Data')\n",
        "        plt.plot(forecast_dates, forecast_values, label='Forecasted Values', marker='o')\n",
        "        plt.title('ETS Forecast of Delivered Orders for the Month of May 2020')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Number of Orders')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.xticks(rotation=45, ha='right')  # Rotate date labels for readability\n",
        "        plt.show()\n",
        "\n",
        "        print(\"ETS Forecasted Number of Orders for the Month of May 2020:\")\n",
        "        print(forecast_values)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred during ETS model training:\", str(e))\n"
      ],
      "metadata": {
        "id": "4CEUlf412GAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prophet: Prophet is a forecasting library developed by Facebook that is designed to handle time series data with strong seasonality and multiple seasonal components.\n",
        "\n",
        "Machine Learning Algorithms: You can also explore machine learning algorithms like Random Forests, Gradient Boosting, or LSTM (Long Short-Term Memory) networks, which are capable of learning complex patterns in time series data.\n",
        "\n",
        "State Space Models: State space models can be used to capture both observed and unobserved components of time series data. Kalman filter-based methods are commonly used in state space models.\n",
        "\n",
        "Gaussian Processes: Gaussian Processes are a powerful non-parametric approach for time series forecasting that can model uncertainty and handle irregularly spaced data."
      ],
      "metadata": {
        "id": "6qWAM_E727gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find the cancellation rate"
      ],
      "metadata": {
        "id": "LCA1MWgE2-iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 0: Load the preprocessed data into a DataFrame\n",
        "chunk_size = 10000\n",
        "data_chunks = pd.read_csv('/content/preprocessed_data.csv', chunksize=chunk_size)\n",
        "data = pd.concat(data_chunks)\n",
        "\n",
        "# Step 1: Replace 'CANCELLED' with 'cancelled'\n",
        "\n",
        "data['status'] = data['status'].replace('CANCELLED', 'cancelled')\n",
        "\n",
        "# Step 2: Calculate the Cancellation Rate\n",
        "\n",
        "# Calculate the total number of orders\n",
        "total_orders = data['order_id'].nunique()\n",
        "\n",
        "# Calculate the number of cancelled orders\n",
        "cancelled_orders = data[data['status'] == 'cancelled']['order_id'].nunique()\n",
        "\n",
        "# Calculate the cancellation rate\n",
        "cancellation_rate = cancelled_orders / total_orders * 100\n",
        "\n",
        "print(\"Cancellation Rate: {:.2f}%\".format(cancellation_rate))\n",
        "\n",
        "# Step 3: Analyze the Reasons for Cancellations\n",
        "\n",
        "# Group data by 'cancelled_by' to count cancellations by each category\n",
        "cancellation_reasons = data[data['status'] == 'cancelled'].groupby('cancelled_by').size()\n",
        "\n",
        "# Print the number of orders canceled for specific reasons\n",
        "print(\"Number of Orders Canceled for Specific Reasons:\")\n",
        "print(cancellation_reasons)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cm2g3j-71Qet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpreting the results and understanding the potential reasons for the cancellations can provide valuable insights into the performance and operational aspects of the on-demand delivery and driver service company. Here's a breakdown of the results and potential reasons:\n",
        "\n",
        "Cancellation Rate: The overall cancellation rate is 43.88%, which means that approximately 43.88% of the total orders placed were canceled at some point before or after the driver accepted the trip or order. A high cancellation rate can indicate potential issues affecting customer satisfaction, delivery efficiency, and driver performance.\n",
        "\n",
        "Number of Orders Canceled for Specific Reasons:\n",
        "\n",
        "Biker Cancellations: 17,038 orders were canceled by bikers. Potential reasons for this could be bikers declining orders due to long distances, unfavorable delivery locations, or personal reasons. The company may need to incentivize bikers for accepting more orders or implement a better system for order assignment to optimize the acceptance rate.\n",
        "\n",
        "Customer Cancellations: 115,063 orders were canceled by customers. Common reasons for this could include changes in delivery requirements, sudden unavailability of customers to receive the order, or customer dissatisfaction with delivery timing or service. Improving communication with customers and providing accurate delivery time estimates may help reduce customer cancellations.\n",
        "\n",
        "System Cancellations: 32,483 orders were canceled by the system. System cancellations may occur due to technical issues, errors, or automated checks to ensure order quality. The company should investigate the reasons for system cancellations and improve the system's reliability and accuracy.\n",
        "\n",
        "Unknown Cancellations: There are 11 orders with an unknown cancellation reason. These cases may require further investigation to identify the reasons behind the cancellations. It's essential to track and understand any unknown cancellations to address potential gaps in data or system issues.\n",
        "\n",
        "Potential Actions and Recommendations:\n",
        "\n",
        "Optimize Biker Assignment: The company can implement an efficient order assignment system that considers bikers' locations, availability, and preferences, thereby reducing biker cancellations.\n",
        "\n",
        "Improve Customer Communication: Enhancing communication channels with customers can help manage their expectations better, reduce surprises, and minimize customer cancellations.\n",
        "\n",
        "Real-Time Order Tracking: Providing real-time tracking and updates to customers can instill confidence and reduce uncertainties, potentially leading to fewer cancellations.\n",
        "\n",
        "Address Technical Issues: The company should closely monitor and address system-related cancellations to improve the platform's stability and reliability.\n",
        "\n",
        "Analyze Historical Data: Analyzing historical data can provide additional insights into patterns and trends related to cancellations. The company can use this information to identify recurring issues and implement targeted solutions.\n",
        "\n",
        "Evaluate Incentive Programs: The company may introduce incentive programs to encourage both bikers and customers to commit to their orders, potentially reducing cancellations.\n",
        "\n",
        "Feedback Collection: Collecting feedback from customers, bikers, and drivers can provide valuable insights into potential areas of improvement.\n",
        "\n",
        "Overall, reducing the cancellation rate and understanding the reasons for cancellations can lead to enhanced operational efficiency, improved customer satisfaction, and increased reliability for the on-demand delivery and driver service company. It is essential to take a proactive approach to address these challenges and continuously optimize the delivery process."
      ],
      "metadata": {
        "id": "wTaHzEUVANjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! To further investigate and understand the cancellation problem, the on-demand delivery and driver service company could provide additional data that can offer deeper insights into the reasons for cancellations. Here are some suggested data points that can help in root cause analysis:\n",
        "\n",
        "1. Biker Performance Metrics: Data related to biker performance metrics can be valuable. This may include biker acceptance rates, average delivery time, customer ratings, number of completed orders, and feedback from customers and bikers. Analyzing biker performance can help identify potential issues with specific bikers or areas where improvements can be made to enhance the delivery process.\n",
        "\n",
        "2. Order Details: More detailed information about the orders can provide context for cancellations. This may include the type of items being delivered, the distance and complexity of the delivery route, delivery time windows, and any special instructions or requirements from customers. Understanding specific order attributes can highlight patterns related to cancellations.\n",
        "\n",
        "3. Customer Feedback: Gathering feedback from customers who canceled their orders can be crucial. Surveys or follow-up communication with customers can provide insights into their reasons for canceling and identify areas for improvement in service quality or communication.\n",
        "\n",
        "4. Biker Feedback: Obtaining feedback from bikers who canceled orders can also be valuable. Bikers might have insights into challenges they faced, such as traffic issues, personal constraints, or difficulties in locating the delivery address.\n",
        "\n",
        "5. Weather Data: Weather conditions can impact delivery logistics. Having weather data for each canceled order's location and time can help understand if weather-related issues contributed to cancellations.\n",
        "\n",
        "6. Order Assignment Algorithm Data: Information on the order assignment algorithm, such as how orders are distributed among bikers, any preferences or constraints set in the system, and the criteria used for assignment, can help in identifying potential issues with the current system.\n",
        "\n",
        "7. Customer Behavior Data: Analyzing customer behavior patterns, such as peak ordering times, repeat orders, or order frequency, can provide insights into potential factors affecting cancellations.\n",
        "\n",
        "8. Delivery Route Data: Data on actual delivery routes taken by bikers can be useful in identifying inefficiencies or challenges in the delivery process.\n",
        "\n",
        "9. Customer Demographics: Understanding customer demographics, such as location, age, or ordering frequency, can help identify if cancellations are more common among certain customer segments.\n",
        "\n",
        "10. Customer Support Interactions: Data on customer support interactions related to cancellations can shed light on common customer concerns and the effectiveness of support responses.\n",
        "\n",
        "By combining the above data with the existing dataset, the on-demand delivery company can conduct more in-depth analyses and identify specific factors contributing to cancellations. Root cause analysis based on this comprehensive data can lead to targeted solutions, process improvements, and a better customer and biker experience."
      ],
      "metadata": {
        "id": "K0WerZydA9Hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Step 0: Load the preprocessed data into a DataFrame\n",
        "chunk_size = 10000\n",
        "data_chunks = pd.read_csv('/content/preprocessed_data.csv', chunksize=chunk_size)\n",
        "data = pd.concat(data_chunks)\n",
        "\n",
        "# Step 1: Data Preparation for RFM Analysis\n",
        "\n",
        "# Convert 'create_time' to datetime object\n",
        "data['create_time'] = pd.to_datetime(data['create_time'])\n",
        "\n",
        "# Filter data for the month of April 2020\n",
        "data_april = data[(data['create_time'] >= '2020-04-01') & (data['create_time'] <= '2020-04-30')]\n",
        "\n",
        "# Step 2: Calculate RFM Metrics for Drivers and Bikers\n",
        "\n",
        "# Calculate Recency: How many days have passed since the last day of the trip for each driver/biker\n",
        "recency_df = data_april.groupby('biker_id')['create_time'].max().reset_index()\n",
        "recency_df['recency'] = (data_april['create_time'].max() - recency_df['create_time']).dt.days\n",
        "\n",
        "# Calculate Frequency: How many days that each driver/biker has tripped in April\n",
        "frequency_df = data_april.groupby('biker_id')['create_time'].nunique().reset_index()\n",
        "frequency_df.rename(columns={'create_time': 'frequency'}, inplace=True)\n",
        "\n",
        "# Calculate Monetary: How many trips each driver/biker has made in April\n",
        "monetary_df = data_april.groupby('biker_id')['order_id'].nunique().reset_index()\n",
        "monetary_df.rename(columns={'order_id': 'monetary'}, inplace=True)\n",
        "\n",
        "# Step 3: Merge the RFM Metrics\n",
        "\n",
        "rfm_df = recency_df.merge(frequency_df, on='biker_id').merge(monetary_df, on='biker_id')\n",
        "\n",
        "# Step 4: RFM Analysis Interpretation\n",
        "\n",
        "# RFM Score Calculation: Assign a score for each RFM metric based on quartiles\n",
        "rfm_df['recency_score'] = pd.qcut(rfm_df['recency'], q=4, labels=False, duplicates='drop')\n",
        "rfm_df['frequency_score'] = pd.qcut(rfm_df['frequency'], q=4, labels=False, duplicates='drop')\n",
        "rfm_df['monetary_score'] = pd.qcut(rfm_df['monetary'], q=4, labels=False, duplicates='drop')\n",
        "\n",
        "# Calculate the RFM score by combining the individual scores (Recency, Frequency, Monetary)\n",
        "rfm_df['rfm_score'] = rfm_df['recency_score'] + rfm_df['frequency_score'] + rfm_df['monetary_score']\n",
        "\n",
        "# Step 5: Cluster Analysis (K-Means Clustering)\n",
        "\n",
        "# Select RFM metrics for clustering\n",
        "rfm_data = rfm_df[['recency', 'frequency', 'monetary']]\n",
        "\n",
        "# Perform k-means clustering with 4 clusters (you can choose a different number of clusters)\n",
        "num_clusters = 4\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "rfm_df['cluster'] = kmeans.fit_predict(rfm_data)\n",
        "\n",
        "# Step 6: Results and Analysis\n",
        "\n",
        "# Print the RFM DataFrame with cluster information for drivers and bikers\n",
        "print(\"RFM Analysis for Drivers and Bikers:\")\n",
        "print(rfm_df)\n",
        "\n",
        "# Summary Statistics\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(rfm_df[['recency', 'frequency', 'monetary', 'rfm_score']].describe())\n",
        "\n",
        "# Visualize RFM Scores by Cluster\n",
        "plt.figure(figsize=(10, 6))\n",
        "palette = sns.color_palette('Set1', n_colors=num_clusters)\n",
        "for cluster in range(num_clusters):\n",
        "    sns.histplot(rfm_df[rfm_df['cluster'] == cluster]['rfm_score'], bins=16, kde=True, color=palette[cluster], label=f'Cluster {cluster}')\n",
        "plt.title('Distribution of RFM Scores by Cluster')\n",
        "plt.xlabel('RFM Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5jEz3EsGAMfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset_ABtesting CSV file\n",
        "data_AB = pd.read_csv('/content/dataset_ABtesting.csv')\n",
        "\n",
        "# Step 1: Group the data by city\n",
        "grouped_data = data_AB.groupby('city')\n",
        "\n",
        "# Step 2: Calculate the percentage of \"biker_id\" for Group 'A' in each city\n",
        "percentage_A = grouped_data.apply(lambda x: x['biker_id'].nunique() / data_AB['biker_id'].nunique())\n",
        "\n",
        "# Step 3: Fill the 'AB_Testing' column with 'A' or 'B' randomly based on the calculated percentages\n",
        "for city, city_group in grouped_data:\n",
        "    mask_A = city_group.index[city_group.index.isin(data_AB.index) & (np.random.rand(len(city_group)) < percentage_A[city])]\n",
        "    mask_B = city_group.index[city_group.index.isin(data_AB.index) & ~city_group.index.isin(mask_A)]\n",
        "    data_AB.loc[mask_A, 'AB_Testing'] = 'A'\n",
        "    data_AB.loc[mask_B, 'AB_Testing'] = 'B'\n",
        "\n",
        "# Step 4: Check and modify the 'AB_Testing' column to satisfy the second condition for each city\n",
        "for city, city_group in grouped_data:\n",
        "    ratio_A = city_group['biker_id'].nunique() / data_AB['biker_id'].nunique()\n",
        "    while not (0.5 < ratio_A < 0.9):\n",
        "        if ratio_A < 0.5:\n",
        "            mask_A = city_group['AB_Testing'] == 'A'\n",
        "            data_AB.loc[city_group[mask_A].index[0], 'AB_Testing'] = 'B'\n",
        "        else:\n",
        "            mask_B = city_group['AB_Testing'] == 'B'\n",
        "            data_AB.loc[city_group[mask_B].index[0], 'AB_Testing'] = 'A'\n",
        "        ratio_A = city_group['biker_id'].nunique() / data_AB['biker_id'].nunique()\n",
        "\n",
        "# Step 5: Check and modify the 'AB_Testing' column to satisfy the first condition for each city\n",
        "for city, city_group in grouped_data:\n",
        "    avg_gt_A = city_group[city_group['AB_Testing'] == 'A']['GT'].mean()\n",
        "    avg_gt_B = city_group[city_group['AB_Testing'] == 'B']['GT'].mean()\n",
        "    while not (-0.04 < (avg_gt_A / avg_gt_B - 1) < -0.01):\n",
        "        if avg_gt_A / avg_gt_B < 1:\n",
        "            mask_A = city_group['AB_Testing'] == 'A'\n",
        "            data_AB.loc[city_group[mask_A].index[0], 'AB_Testing'] = 'B'\n",
        "        else:\n",
        "            mask_B = city_group['AB_Testing'] == 'B'\n",
        "            data_AB.loc[city_group[mask_B].index[0], 'AB_Testing'] = 'A'\n",
        "        avg_gt_A = city_group[city_group['AB_Testing'] == 'A']['GT'].mean()\n",
        "        avg_gt_B = city_group[city_group['AB_Testing'] == 'B']['GT'].mean()\n",
        "\n",
        "# Write the output to ABtesting_result.csv\n",
        "data_AB.to_csv('/content/ABtesting_result.csv', index=False)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(data_AB)\n"
      ],
      "metadata": {
        "id": "KyMZXYR2ZVuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Load the dataset_ABtesting CSV file\n",
        "data_AB = pd.read_csv('/content/dataset_ABtesting.csv')\n",
        "\n",
        "# Step 1: Define the Chromosome Representation\n",
        "def initialize_chromosome(num_rows):\n",
        "    return ''.join(random.choice(['A', 'B']) for _ in range(num_rows))\n",
        "\n",
        "# Step 2: Define the Fitness Function\n",
        "def fitness_function(chromosome, data):\n",
        "    # Convert the chromosome to 'AB_Testing' column values\n",
        "    data['AB_Testing'] = list(chromosome)\n",
        "\n",
        "    # Calculate the percentage of 'A' biker_id and average 'GT' ratio for each city group\n",
        "    grouped_data = data.groupby('city')\n",
        "    percentage_A = grouped_data['biker_id'].nunique() / data['biker_id'].nunique()\n",
        "    avg_gt_ratio = grouped_data.apply(lambda group: group.loc[group['AB_Testing'] == 'A', 'GT'].mean() /\n",
        "                                               group.loc[group['AB_Testing'] == 'B', 'GT'].mean() - 1)\n",
        "\n",
        "    # Calculate the fitness value as a combination of differences from target values in both conditions\n",
        "    fitness_A = np.mean(np.abs(percentage_A - 0.7))  # Target percentage_A: 70%\n",
        "    fitness_GT = np.mean(np.abs(avg_gt_ratio - (-0.025)))  # Target average GT ratio: -0.025\n",
        "    return fitness_A + fitness_GT\n",
        "\n",
        "# Step 3: Initialize the Population\n",
        "def initialize_population(pop_size, num_rows):\n",
        "    return [initialize_chromosome(num_rows) for _ in range(pop_size)]\n",
        "\n",
        "# Step 4: Apply Genetic Operators\n",
        "def crossover(parent1, parent2):\n",
        "    crossover_point = random.randint(1, len(parent1) - 1)\n",
        "    child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
        "    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
        "    return child1, child2\n",
        "\n",
        "def mutation(chromosome, mutation_rate):\n",
        "    return ''.join(bit if random.random() > mutation_rate else random.choice(['A', 'B']) for bit in chromosome)\n",
        "\n",
        "# Step 5: Evaluate the Population\n",
        "def evaluate_population(population, data):\n",
        "    return [fitness_function(chromosome, data) for chromosome in population]\n",
        "\n",
        "# Step 6: Select Parents for Reproduction\n",
        "def tournament_selection(population, fitness_values, tournament_size):\n",
        "    tournament_indices = random.sample(range(len(population)), tournament_size)\n",
        "    tournament_fitness = [fitness_values[i] for i in tournament_indices]\n",
        "    winner_index = tournament_indices[np.argmin(tournament_fitness)]\n",
        "    return population[winner_index]\n",
        "\n",
        "# Step 7: Apply Genetic Operators to Create New Generation\n",
        "def generate_new_population(population, fitness_values, mutation_rate, tournament_size):\n",
        "    new_population = []\n",
        "    while len(new_population) < len(population):\n",
        "        parent1 = tournament_selection(population, fitness_values, tournament_size)\n",
        "        parent2 = tournament_selection(population, fitness_values, tournament_size)\n",
        "        child1, child2 = crossover(parent1, parent2)\n",
        "        child1 = mutation(child1, mutation_rate)\n",
        "        child2 = mutation(child2, mutation_rate)\n",
        "        new_population.extend([child1, child2])\n",
        "    return new_population[:len(population)]\n",
        "\n",
        "# Step 8: Repeat Steps 5-7 for Several Generations\n",
        "def genetic_algorithm(data, pop_size, num_generations, mutation_rate, tournament_size):\n",
        "    num_rows = len(data)\n",
        "    population = initialize_population(pop_size, num_rows)\n",
        "\n",
        "    for generation in range(num_generations):\n",
        "        fitness_values = evaluate_population(population, data)\n",
        "        best_fitness = min(fitness_values)\n",
        "        best_chromosome = population[np.argmin(fitness_values)]\n",
        "        print(f\"Generation {generation + 1} - Best Fitness: {best_fitness:.4f}\")\n",
        "\n",
        "        if best_fitness == 0.0:\n",
        "            break\n",
        "\n",
        "        population = generate_new_population(population, fitness_values, mutation_rate, tournament_size)\n",
        "\n",
        "    return best_chromosome\n",
        "\n",
        "# Step 9: Extract the Best Solution and Update the 'AB_Testing' Column\n",
        "best_chromosome = genetic_algorithm(data_AB, pop_size=100, num_generations=100, mutation_rate=0.02, tournament_size=10)\n",
        "data_AB['AB_Testing'] = list(best_chromosome)\n",
        "\n",
        "# Step 10: Write the output to ABtesting_result.csv\n",
        "data_AB.to_csv('/content/ABtesting_result.csv', index=False)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(data_AB)\n"
      ],
      "metadata": {
        "id": "r38FUpZ-exY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Load the dataset_ABtesting CSV file\n",
        "data_AB = pd.read_csv('/content/dataset_ABtesting.csv')\n",
        "\n",
        "# Assuming 'dataset' is your DataFrame\n",
        "# Check for missing values in each column\n",
        "missing_values = data_AB.isnull().sum()\n",
        "\n",
        "# Print the number of missing values in each column\n",
        "print(missing_values)\n",
        "\n",
        "\n",
        "# Assuming 'dataset' is your DataFrame\n",
        "# Check for duplicate rows\n",
        "duplicate_rows = data_AB.duplicated()\n",
        "\n",
        "# Print the number of duplicate rows\n",
        "print(\"Number of duplicate rows:\", duplicate_rows.sum())\n",
        "\n",
        "# Step 1: Define the Chromosome Representation\n",
        "def initialize_chromosome(num_rows):\n",
        "    return ''.join(random.choice(['A', 'B']) for _ in range(num_rows))\n",
        "\n",
        "# Step 2: Define the Fitness Function\n",
        "def fitness_function(chromosome, data):\n",
        "    # Convert the chromosome to 'AB_Testing' column values\n",
        "    data['AB_Testing'] = list(chromosome)\n",
        "\n",
        "    # Calculate the percentage of 'A' biker_id and average 'GT' ratio for each city group\n",
        "    grouped_data = data.groupby('city')\n",
        "    percentage_A = grouped_data['biker_id'].nunique() / data['biker_id'].nunique()\n",
        "    avg_gt_ratio = grouped_data.apply(lambda group: group.loc[group['AB_Testing'] == 'A', 'GT'].mean() /\n",
        "                                               group.loc[group['AB_Testing'] == 'B', 'GT'].mean())\n",
        "\n",
        "    # Calculate the fitness value as a combination of differences from target values in both conditions\n",
        "    fitness_A = np.mean(np.abs(percentage_A - 0.7))  # Target percentage_A: 70%\n",
        "    fitness_GT = np.mean(np.abs(avg_gt_ratio - 0.97))  # Target average GT ratio: 0.97\n",
        "    return fitness_A + fitness_GT\n",
        "\n",
        "# Step 3: Initialize the Population\n",
        "def initialize_population(pop_size, num_rows):\n",
        "    return [initialize_chromosome(num_rows) for _ in range(pop_size)]\n",
        "\n",
        "# Step 4: Apply Genetic Operators\n",
        "def crossover(parent1, parent2, crossover_rate):\n",
        "    if random.random() < crossover_rate:\n",
        "        crossover_point = random.randint(1, len(parent1) - 1)\n",
        "        child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
        "        child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
        "        return child1, child2\n",
        "    else:\n",
        "        return parent1, parent2\n",
        "\n",
        "def mutation(chromosome, mutation_rate):\n",
        "    return ''.join(bit if random.random() > mutation_rate else random.choice(['A', 'B']) for bit in chromosome)\n",
        "\n",
        "# Step 5: Evaluate the Population\n",
        "def evaluate_population(population, data):\n",
        "    return [fitness_function(chromosome, data) for chromosome in population]\n",
        "\n",
        "# Step 6: Select Parents for Reproduction\n",
        "def tournament_selection(population, fitness_values, tournament_size):\n",
        "    tournament_indices = random.sample(range(len(population)), tournament_size)\n",
        "    tournament_fitness = [fitness_values[i] for i in tournament_indices]\n",
        "    winner_index = tournament_indices[np.argmin(tournament_fitness)]\n",
        "    return population[winner_index]\n",
        "\n",
        "# Step 7: Apply Genetic Operators to Create New Generation\n",
        "def generate_new_population(population, fitness_values, mutation_rate, tournament_size, crossover_rate):\n",
        "    new_population = []\n",
        "    while len(new_population) < len(population):\n",
        "        parent1 = tournament_selection(population, fitness_values, tournament_size)\n",
        "        parent2 = tournament_selection(population, fitness_values, tournament_size)\n",
        "        child1, child2 = crossover(parent1, parent2, crossover_rate)\n",
        "        child1 = mutation(child1, mutation_rate)\n",
        "        child2 = mutation(child2, mutation_rate)\n",
        "        new_population.extend([child1, child2])\n",
        "    return new_population[:len(population)]\n",
        "\n",
        "# Step 8: Repeat Steps 5-7 for Several Generations\n",
        "def genetic_algorithm(data, pop_size, num_generations, mutation_rate, tournament_size, crossover_rate):\n",
        "    num_rows = len(data)\n",
        "    population = initialize_population(pop_size, num_rows)\n",
        "\n",
        "    for generation in range(num_generations):\n",
        "        fitness_values = evaluate_population(population, data)\n",
        "        best_fitness = min(fitness_values)\n",
        "        best_chromosome = population[np.argmin(fitness_values)]\n",
        "        print(f\"Generation {generation + 1} - Best Fitness: {best_fitness:.4f}\")\n",
        "\n",
        "        if best_fitness == 0.0:\n",
        "            break\n",
        "\n",
        "        population = generate_new_population(population, fitness_values, mutation_rate, tournament_size, crossover_rate)\n",
        "\n",
        "    return best_chromosome\n",
        "\n",
        "# Step 9: Extract the Best Solution and Update the 'AB_Testing' Column\n",
        "crossover_rate = 0.8  # Set your desired crossover rate here\n",
        "best_chromosome = genetic_algorithm(data_AB, pop_size=1000, num_generations=1000, mutation_rate=0.02, tournament_size=10, crossover_rate=crossover_rate)\n",
        "data_AB['AB_Testing'] = list(best_chromosome)\n",
        "\n",
        "# Step 10: Write the output to ABtesting_result.csv\n",
        "data_AB.to_csv('/content/ABtesting_result.csv', index=False)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(data_AB)\n"
      ],
      "metadata": {
        "id": "f_IxncyK1HoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Load the dataset_ABtesting CSV file\n",
        "data_AB = pd.read_csv('/content/dataset_ABtesting.csv')\n",
        "\n",
        "# Step 1: Define the Chromosome Representation\n",
        "def initialize_chromosome(num_rows):\n",
        "    return ''.join(random.choice(['A', 'B']) for _ in range(num_rows))\n",
        "\n",
        "# Step 2: Define the Fitness Function\n",
        "def fitness_function(chromosome, data):\n",
        "    # Convert the chromosome to 'AB_Testing' column values\n",
        "    data['AB_Testing'] = list(chromosome)\n",
        "\n",
        "    # Calculate the percentage of 'A' biker_id and average 'GT' ratio for each city group\n",
        "    grouped_data = data.groupby('city')\n",
        "    percentage_A = grouped_data['biker_id'].nunique() / data['biker_id'].nunique()\n",
        "    avg_gt_ratio = grouped_data.apply(lambda group: group.loc[group['AB_Testing'] == 'A', 'GT'].mean() /\n",
        "                                               group.loc[group['AB_Testing'] == 'B', 'GT'].mean())\n",
        "\n",
        "    # Calculate the fitness value as a combination of differences from target values in both conditions\n",
        "    fitness_A = np.mean(np.abs(percentage_A - 0.7))  # Target percentage_A: 70%\n",
        "    fitness_GT = np.mean(np.abs(avg_gt_ratio - 0.97))  # Target average GT ratio: 0.97\n",
        "    return fitness_A + fitness_GT\n",
        "\n",
        "# Step 3: Initialize the Population\n",
        "def initialize_population(pop_size, num_rows):\n",
        "    return [initialize_chromosome(num_rows) for _ in range(pop_size)]\n",
        "\n",
        "# Step 4: Apply Genetic Operators\n",
        "def crossover(parent1, parent2, crossover_rate):\n",
        "    if random.random() < crossover_rate:\n",
        "        crossover_point = random.randint(1, len(parent1) - 1)\n",
        "        child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
        "        child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
        "        return child1, child2\n",
        "    else:\n",
        "        return parent1, parent2\n",
        "\n",
        "def mutation(chromosome, mutation_rate):\n",
        "    return ''.join(bit if random.random() > mutation_rate else random.choice(['A', 'B']) for bit in chromosome)\n",
        "\n",
        "# Step 5: Evaluate the Population\n",
        "def evaluate_population(population, data):\n",
        "    return [fitness_function(chromosome, data) for chromosome in population]\n",
        "\n",
        "# Step 6: Select Parents for Reproduction\n",
        "def tournament_selection(population, fitness_values, tournament_size):\n",
        "    tournament_indices = random.sample(range(len(population)), tournament_size)\n",
        "    tournament_fitness = [fitness_values[i] for i in tournament_indices]\n",
        "    winner_index = tournament_indices[np.argmin(tournament_fitness)]\n",
        "    return population[winner_index]\n",
        "\n",
        "# Step 7: Apply Genetic Operators to Create New Generation\n",
        "def generate_new_population(population, fitness_values, mutation_rate, tournament_size, crossover_rate):\n",
        "    new_population = []\n",
        "    while len(new_population) < len(population):\n",
        "        parent1 = tournament_selection(population, fitness_values, tournament_size)\n",
        "        parent2 = tournament_selection(population, fitness_values, tournament_size)\n",
        "        child1, child2 = crossover(parent1, parent2, crossover_rate)\n",
        "        child1 = mutation(child1, mutation_rate)\n",
        "        child2 = mutation(child2, mutation_rate)\n",
        "        new_population.extend([child1, child2])\n",
        "    return new_population[:len(population)]\n",
        "\n",
        "# Step 8: Repeat Steps 5-7 for Several Generations with Best Hyperparameters\n",
        "def genetic_algorithm(data, pop_size, num_generations, mutation_rate, tournament_size, crossover_rate):\n",
        "    num_rows = len(data)\n",
        "    population = initialize_population(pop_size, num_rows)\n",
        "\n",
        "    for generation in range(num_generations):\n",
        "        fitness_values = evaluate_population(population, data)\n",
        "        best_fitness = min(fitness_values)\n",
        "        best_chromosome = population[np.argmin(fitness_values)]\n",
        "        print(f\"Generation {generation + 1} - Best Fitness: {best_fitness:.4f}\")\n",
        "\n",
        "        if best_fitness == 0.0:\n",
        "            break\n",
        "\n",
        "        population = generate_new_population(population, fitness_values, mutation_rate, tournament_size, crossover_rate)\n",
        "\n",
        "    return best_chromosome\n",
        "\n",
        "# Step 9: Perform Grid Search for Best Hyperparameters\n",
        "best_hyperparameters = {}\n",
        "best_fitness = float('inf')\n",
        "\n",
        "pop_size_values = [100, 500, 1000]\n",
        "num_generations_values = [500, 1000, 2000]\n",
        "mutation_rate_values = [0.01, 0.02, 0.05]\n",
        "tournament_size_values = [5, 10, 20]\n",
        "crossover_rate_values = [0.7, 0.8, 0.9]\n",
        "\n",
        "for pop_size in pop_size_values:\n",
        "    for num_generations in num_generations_values:\n",
        "        for mutation_rate in mutation_rate_values:\n",
        "            for tournament_size in tournament_size_values:\n",
        "                for crossover_rate in crossover_rate_values:\n",
        "                    print(f\"Testing hyperparameters: pop_size={pop_size}, num_generations={num_generations}, \"\n",
        "                          f\"mutation_rate={mutation_rate}, tournament_size={tournament_size}, \"\n",
        "                          f\"crossover_rate={crossover_rate}\")\n",
        "\n",
        "                    chromosome = genetic_algorithm(data_AB, pop_size=pop_size, num_generations=num_generations,\n",
        "                                                    mutation_rate=mutation_rate, tournament_size=tournament_size,\n",
        "                                                    crossover_rate=crossover_rate)\n",
        "                    fitness_value = fitness_function(chromosome, data_AB)\n",
        "\n",
        "                    if fitness_value < best_fitness:\n",
        "                        best_hyperparameters = {\n",
        "                            'pop_size': pop_size,\n",
        "                            'num_generations': num_generations,\n",
        "                            'mutation_rate': mutation_rate,\n",
        "                            'tournament_size': tournament_size,\n",
        "                            'crossover_rate': crossover_rate\n",
        "                        }\n",
        "                        best_fitness = fitness_value\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
        "\n",
        "# Step 10: Run Genetic Algorithm with Best Hyperparameters and Update 'AB_Testing' Column\n",
        "best_chromosome = genetic_algorithm(data_AB, **best_hyperparameters)\n",
        "data_AB['AB_Testing'] = list(best_chromosome)\n",
        "\n",
        "# Write the output to ABtesting_result.csv\n",
        "data_AB.to_csv('/content/ABtesting_result.csv', index=False)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(data_AB)\n"
      ],
      "metadata": {
        "id": "ILTyQSt2239X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Load the dataset_ABtesting CSV file\n",
        "data_AB = pd.read_csv('/content/dataset_ABtesting.csv')\n",
        "\n",
        "# Assuming 'dataset' is your DataFrame\n",
        "# Check for missing values in each column\n",
        "missing_values = data_AB.isnull().sum()\n",
        "\n",
        "# Print the number of missing values in each column\n",
        "print(missing_values)\n",
        "\n",
        "# Assuming 'dataset' is your DataFrame\n",
        "# Check for duplicate rows\n",
        "duplicate_rows = data_AB.duplicated()\n",
        "\n",
        "# Print the number of duplicate rows\n",
        "print(\"Number of duplicate rows:\", duplicate_rows.sum())\n",
        "\n",
        "# Step 1: Define the Chromosome Representation\n",
        "def initialize_chromosome(num_rows):\n",
        "    return ''.join(random.choice(['A', 'B']) for _ in range(num_rows))\n",
        "\n",
        "# Step 2: Define the Fitness Function\n",
        "def fitness_function(chromosome, data):\n",
        "    # Convert the chromosome to 'AB_Testing' column values\n",
        "    data['AB_Testing'] = list(chromosome)\n",
        "\n",
        "    # Calculate the percentage of 'A' biker_id and average 'GT' ratio for each city group\n",
        "    grouped_data = data.groupby('city')\n",
        "    percentage_A = grouped_data['biker_id'].nunique() / data['biker_id'].nunique()\n",
        "    avg_gt_ratio = grouped_data.apply(lambda group: group.loc[group['AB_Testing'] == 'A', 'GT'].mean() /\n",
        "                                               group.loc[group['AB_Testing'] == 'B', 'GT'].mean())\n",
        "\n",
        "    # Calculate the fitness value as a combination of differences from target values in both conditions\n",
        "    fitness_A = np.mean(np.abs(percentage_A - 0.7))  # Target percentage_A: 70%\n",
        "    fitness_GT = np.mean(np.abs(avg_gt_ratio - 0.97))  # Target average GT ratio: 0.97\n",
        "    return fitness_A + fitness_GT\n",
        "\n",
        "# Step 3: Initialize the Population\n",
        "def initialize_population(pop_size, num_rows):\n",
        "    return [initialize_chromosome(num_rows) for _ in range(pop_size)]\n",
        "\n",
        "# Step 4: Apply Genetic Operators\n",
        "def crossover(parent1, parent2, crossover_rate):\n",
        "    if random.random() < crossover_rate:\n",
        "        crossover_point = random.randint(1, len(parent1) - 1)\n",
        "        child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
        "        child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
        "        return child1, child2\n",
        "    else:\n",
        "        return parent1, parent2\n",
        "\n",
        "def mutation(chromosome, mutation_rate):\n",
        "    return ''.join(bit if random.random() > mutation_rate else random.choice(['A', 'B']) for bit in chromosome)\n",
        "\n",
        "# Step 5: Evaluate the Population\n",
        "def evaluate_population(population, data):\n",
        "    return [fitness_function(chromosome, data) for chromosome in population]\n",
        "\n",
        "# Step 6: Select Parents for Reproduction\n",
        "def tournament_selection(population, fitness_values, tournament_size):\n",
        "    tournament_indices = random.sample(range(len(population)), tournament_size)\n",
        "    tournament_fitness = [fitness_values[i] for i in tournament_indices]\n",
        "    winner_index = tournament_indices[np.argmin(tournament_fitness)]\n",
        "    return population[winner_index]\n",
        "\n",
        "# Step 7: Apply Genetic Operators to Create New Generation\n",
        "def generate_new_population(population, fitness_values, mutation_rate, tournament_size, crossover_rate):\n",
        "    new_population = []\n",
        "    while len(new_population) < len(population):\n",
        "        parent1 = tournament_selection(population, fitness_values, tournament_size)\n",
        "        parent2 = tournament_selection(population, fitness_values, tournament_size)\n",
        "        child1, child2 = crossover(parent1, parent2, crossover_rate)\n",
        "        child1 = mutation(child1, mutation_rate)\n",
        "        child2 = mutation(child2, mutation_rate)\n",
        "        new_population.extend([child1, child2])\n",
        "    return new_population[:len(population)]\n",
        "\n",
        "# Step 11: Implement Elitism\n",
        "def elitism(population, fitness_values):\n",
        "    elite_index = np.argmin(fitness_values)\n",
        "    elite_chromosome = population[elite_index]\n",
        "    return elite_chromosome\n",
        "\n",
        "# Step 8: Repeat Steps 5-7 for Several Generations with Elitism\n",
        "def genetic_algorithm_with_elitism(data, pop_size, num_generations, mutation_rate, tournament_size, crossover_rate):\n",
        "    num_rows = len(data)\n",
        "    population = initialize_population(pop_size, num_rows)\n",
        "    best_chromosome = None\n",
        "\n",
        "    for generation in range(num_generations):\n",
        "        fitness_values = evaluate_population(population, data)\n",
        "        best_fitness = min(fitness_values)\n",
        "        best_chromosome = population[np.argmin(fitness_values)]\n",
        "        print(f\"Generation {generation + 1} - Best Fitness: {best_fitness:.4f}\")\n",
        "\n",
        "        if best_fitness == 0.0:\n",
        "            break\n",
        "\n",
        "        elite_chromosome = elitism(population, fitness_values)\n",
        "        population = generate_new_population(population, fitness_values, mutation_rate, tournament_size, crossover_rate)\n",
        "        population[-1] = elite_chromosome  # Replace the last individual with the elite\n",
        "\n",
        "    return best_chromosome\n",
        "\n",
        "# Step 9: Extract the Best Solution and Update the 'AB_Testing' Column with Elitism\n",
        "crossover_rate = 0.8  # Set your desired crossover rate here\n",
        "best_chromosome = genetic_algorithm_with_elitism(data_AB, pop_size=1000, num_generations=1000, mutation_rate=0.02, tournament_size=10, crossover_rate=crossover_rate)\n",
        "data_AB['AB_Testing'] = list(best_chromosome)\n",
        "\n",
        "# Step 10: Write the output to ABtesting_result.csv\n",
        "data_AB.to_csv('/content/ABtesting_result.csv', index=False)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(data_AB)\n"
      ],
      "metadata": {
        "id": "IR8HBzNoDYeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GN5ZAPoefnk",
        "outputId": "69782b04-e67f-40bc-fbc9-fcff192ef864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deap\n",
            "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.22.4)\n",
            "Installing collected packages: deap\n",
            "Successfully installed deap-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from deap import creator, base, tools, algorithms\n",
        "\n",
        "# Load the dataset from CSV\n",
        "data = pd.read_csv('dataset_ABtesting.csv')\n",
        "\n",
        "# Define the constants and target conditions\n",
        "TARGET_AVERAGE_GT_LOWER = 0.94\n",
        "TARGET_AVERAGE_GT_UPPER = 0.99\n",
        "TARGET_BIKER_ID_RATIO_LOWER = 0.5\n",
        "TARGET_BIKER_ID_RATIO_UPPER = 0.9\n",
        "\n",
        "# Genetic Algorithm parameters\n",
        "POPULATION_SIZE = 100\n",
        "GENERATIONS = 100\n",
        "CXPB = 0.8  # Crossover probability\n",
        "MUTPB = 0.02  # Mutation probability\n",
        "\n",
        "# Define the fitness function to evaluate how well the conditions are met for each city\n",
        "def fitness_function(individual):\n",
        "    # Convert the individual (list of 'AB_Testing') to a DataFrame\n",
        "    df = pd.DataFrame({'AB_Testing': individual})\n",
        "    complete_data = pd.concat([data[['biker_id', 'GT', 'city']], df], axis=1)\n",
        "\n",
        "    # Calculate the average 'GT' for group 'A' and group 'B' for each city\n",
        "    city_groups = complete_data.groupby(['city', 'AB_Testing'])\n",
        "    avg_gt_per_city_group = city_groups['GT'].mean().unstack(level=-1)\n",
        "\n",
        "    # Calculate the count of \"biker_id\" for group 'A' and the total count of \"biker_id\" for each city\n",
        "    count_biker_id_per_city_group = city_groups['biker_id'].count().unstack(level=-1)\n",
        "\n",
        "    # Calculate the fitness score for each city\n",
        "    fitness_scores = []\n",
        "    for city in data['city'].unique():\n",
        "        group_a_gt = avg_gt_per_city_group.loc[city, 'A']\n",
        "        group_b_gt = avg_gt_per_city_group.loc[city, 'B']\n",
        "        biker_id_a = count_biker_id_per_city_group.loc[city, 'A']\n",
        "        total_biker_id = count_biker_id_per_city_group.loc[city, 'B'] + biker_id_a\n",
        "\n",
        "        # Calculate the fitness scores for the average 'GT' and count \"biker_id\" conditions\n",
        "        avg_gt_fitness = 1.0 - (abs(group_a_gt / group_b_gt - 1) - TARGET_AVERAGE_GT_LOWER) / (TARGET_AVERAGE_GT_UPPER - TARGET_AVERAGE_GT_LOWER)\n",
        "        biker_id_fitness = 1.0 - (abs(biker_id_a / total_biker_id - 1) - TARGET_BIKER_ID_RATIO_LOWER) / (TARGET_BIKER_ID_RATIO_UPPER - TARGET_BIKER_ID_RATIO_LOWER)\n",
        "\n",
        "        # Calculate the overall fitness score for the city as the product of the two fitness scores\n",
        "        city_fitness = avg_gt_fitness * biker_id_fitness\n",
        "        fitness_scores.append(city_fitness)\n",
        "\n",
        "    # Calculate the overall fitness score for the individual as the average of the city fitness scores\n",
        "    overall_fitness = np.mean(fitness_scores)\n",
        "\n",
        "    # Convert the fitness to a percentage\n",
        "    fitness_percentage = overall_fitness * 100\n",
        "    return fitness_percentage,\n",
        "\n",
        "# Create the DEAP framework components\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "# Define the genetic operators\n",
        "toolbox.register(\"attribute\", random.choice, ['A', 'B'])  # 'AB_Testing' attribute: A or B\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attribute, len(data))\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# Define the evaluation function\n",
        "toolbox.register(\"evaluate\", fitness_function)\n",
        "\n",
        "# Define the genetic operators\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "def main():\n",
        "    # Create the initial population\n",
        "    population = toolbox.population(n=POPULATION_SIZE)\n",
        "\n",
        "    # Evaluate the fitness of each individual in the population\n",
        "    fitnesses = list(map(toolbox.evaluate, population))\n",
        "    for ind, fit in zip(population, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    # Keep track of the best individual\n",
        "    best_individual = tools.selBest(population, k=1)[0]\n",
        "\n",
        "    for gen in range(1, GENERATIONS + 1):\n",
        "        # Select the next generation individuals\n",
        "        offspring = toolbox.select(population, len(population))\n",
        "\n",
        "        # Clone the selected individuals\n",
        "        offspring = list(map(toolbox.clone, offspring))\n",
        "\n",
        "        # Apply crossover and mutation on the offspring\n",
        "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
        "            if random.random() < CXPB:\n",
        "                toolbox.mate(child1, child2)\n",
        "                del child1.fitness.values\n",
        "                del child2.fitness.values\n",
        "\n",
        "        for mutant in offspring:\n",
        "            if random.random() < MUTPB:\n",
        "                toolbox.mutate(mutant)\n",
        "                del mutant.fitness.values\n",
        "\n",
        "        # Evaluate the fitness of the new offspring\n",
        "        fitnesses = list(map(toolbox.evaluate, offspring))\n",
        "        for ind, fit in zip(offspring, fitnesses):\n",
        "            ind.fitness.values = fit\n",
        "\n",
        "        # Replace the current population with the offspring\n",
        "        population[:] = offspring\n",
        "\n",
        "        # Update the best individual\n",
        "        best_individual = tools.selBest(population + [best_individual], k=1)[0]\n",
        "\n",
        "        # Print the best fitness value for each generation\n",
        "        print(f\"Generation {gen}: Best Fitness = {best_individual.fitness.values[0]}\")\n",
        "\n",
        "    # Update the original dataset with the 'AB_Testing' values of the best individual\n",
        "    data['AB_Testing'] = best_individual\n",
        "\n",
        "    # Print the final dataset with the 'AB_Testing' values filled\n",
        "    print(\"\\nFinal Dataset:\")\n",
        "    print(data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89X7gcToegni",
        "outputId": "e8a8877a-5059-4a56-cd22-065c39cec2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 1: Best Fitness = 1955.5678164886313\n",
            "Generation 2: Best Fitness = 2040.578849412982\n",
            "Generation 3: Best Fitness = 2053.674127208077\n",
            "Generation 4: Best Fitness = 2071.530811310096\n",
            "Generation 5: Best Fitness = 2101.853149882991\n",
            "Generation 6: Best Fitness = 2101.853149882991\n",
            "Generation 7: Best Fitness = 2105.4996451614034\n",
            "Generation 8: Best Fitness = 2131.9205365577304\n",
            "Generation 9: Best Fitness = 2167.8176323067723\n",
            "Generation 10: Best Fitness = 2167.8176323067723\n",
            "Generation 11: Best Fitness = 2175.582588659016\n",
            "Generation 12: Best Fitness = 2175.582588659016\n",
            "Generation 13: Best Fitness = 2187.5955478572914\n",
            "Generation 14: Best Fitness = 2200.5736991542803\n",
            "Generation 15: Best Fitness = 2200.5736991542803\n",
            "Generation 16: Best Fitness = 2206.2058831452464\n",
            "Generation 17: Best Fitness = 2212.9352839235917\n",
            "Generation 18: Best Fitness = 2222.001370350041\n",
            "Generation 19: Best Fitness = 2222.001370350041\n",
            "Generation 20: Best Fitness = 2222.4699630477667\n",
            "Generation 21: Best Fitness = 2226.5771985762403\n",
            "Generation 22: Best Fitness = 2238.1912648218376\n",
            "Generation 23: Best Fitness = 2243.4976495593965\n",
            "Generation 24: Best Fitness = 2244.447003489939\n",
            "Generation 25: Best Fitness = 2248.618291642998\n",
            "Generation 26: Best Fitness = 2248.637326688755\n",
            "Generation 27: Best Fitness = 2250.8048471554976\n",
            "Generation 28: Best Fitness = 2261.711225301764\n",
            "Generation 29: Best Fitness = 2261.711225301764\n",
            "Generation 30: Best Fitness = 2262.9614974946185\n",
            "Generation 31: Best Fitness = 2262.9614974946185\n",
            "Generation 32: Best Fitness = 2262.9614974946185\n",
            "Generation 33: Best Fitness = 2264.60609568514\n",
            "Generation 34: Best Fitness = 2265.936030517305\n",
            "Generation 35: Best Fitness = 2265.936030517305\n",
            "Generation 36: Best Fitness = 2265.936030517305\n",
            "Generation 37: Best Fitness = 2266.1010077152505\n",
            "Generation 38: Best Fitness = 2266.1010077152505\n",
            "Generation 39: Best Fitness = 2266.1010077152505\n",
            "Generation 40: Best Fitness = 2266.1010077152505\n",
            "Generation 41: Best Fitness = 2266.1010077152505\n",
            "Generation 42: Best Fitness = 2266.1010077152505\n",
            "Generation 43: Best Fitness = 2266.1010077152505\n",
            "Generation 44: Best Fitness = 2266.1010077152505\n",
            "Generation 45: Best Fitness = 2266.1010077152505\n",
            "Generation 46: Best Fitness = 2266.1010077152505\n",
            "Generation 47: Best Fitness = 2266.1010077152505\n",
            "Generation 48: Best Fitness = 2266.1010077152505\n",
            "Generation 49: Best Fitness = 2266.1010077152505\n",
            "Generation 50: Best Fitness = 2266.1010077152505\n",
            "Generation 51: Best Fitness = 2266.1010077152505\n",
            "Generation 52: Best Fitness = 2266.1010077152505\n",
            "Generation 53: Best Fitness = 2266.1010077152505\n",
            "Generation 54: Best Fitness = 2266.1010077152505\n",
            "Generation 55: Best Fitness = 2266.1010077152505\n",
            "Generation 56: Best Fitness = 2266.1010077152505\n",
            "Generation 57: Best Fitness = 2266.1010077152505\n",
            "Generation 58: Best Fitness = 2266.1010077152505\n",
            "Generation 59: Best Fitness = 2266.1010077152505\n",
            "Generation 60: Best Fitness = 2266.1010077152505\n",
            "Generation 61: Best Fitness = 2266.1010077152505\n",
            "Generation 62: Best Fitness = 2266.1010077152505\n",
            "Generation 63: Best Fitness = 2266.1010077152505\n",
            "Generation 64: Best Fitness = 2266.1010077152505\n",
            "Generation 65: Best Fitness = 2266.1010077152505\n",
            "Generation 66: Best Fitness = 2266.1010077152505\n",
            "Generation 67: Best Fitness = 2266.1010077152505\n",
            "Generation 68: Best Fitness = 2266.1010077152505\n",
            "Generation 69: Best Fitness = 2266.1010077152505\n",
            "Generation 70: Best Fitness = 2266.1010077152505\n",
            "Generation 71: Best Fitness = 2266.1010077152505\n",
            "Generation 72: Best Fitness = 2266.1010077152505\n",
            "Generation 73: Best Fitness = 2266.1010077152505\n",
            "Generation 74: Best Fitness = 2266.1010077152505\n",
            "Generation 75: Best Fitness = 2266.1010077152505\n",
            "Generation 76: Best Fitness = 2266.1010077152505\n",
            "Generation 77: Best Fitness = 2266.1010077152505\n",
            "Generation 78: Best Fitness = 2266.1010077152505\n",
            "Generation 79: Best Fitness = 2266.1010077152505\n",
            "Generation 80: Best Fitness = 2266.1010077152505\n",
            "Generation 81: Best Fitness = 2266.1010077152505\n",
            "Generation 82: Best Fitness = 2266.1010077152505\n",
            "Generation 83: Best Fitness = 2266.1010077152505\n",
            "Generation 84: Best Fitness = 2266.1010077152505\n",
            "Generation 85: Best Fitness = 2266.1010077152505\n",
            "Generation 86: Best Fitness = 2266.1010077152505\n",
            "Generation 87: Best Fitness = 2266.1010077152505\n",
            "Generation 88: Best Fitness = 2266.1010077152505\n",
            "Generation 89: Best Fitness = 2266.1010077152505\n",
            "Generation 90: Best Fitness = 2266.1010077152505\n",
            "Generation 91: Best Fitness = 2266.1010077152505\n",
            "Generation 92: Best Fitness = 2266.1010077152505\n",
            "Generation 93: Best Fitness = 2266.1010077152505\n",
            "Generation 94: Best Fitness = 2266.1010077152505\n",
            "Generation 95: Best Fitness = 2266.1010077152505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "iCNTxO1_f9SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nD8lEqbEf9qM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}